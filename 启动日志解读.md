下面我把日志逐行/按类别解释清楚、指出代码中应改进的具体点并给出可复制的修复建议与代码片段。结论先放前面：**主要问题是模型输出层用 `sigmoid`（对回归不友好）和部分输入/shape 使用不当导致 retracing 警告；还有 IMF1 表现极差，可能是该 IMF 的尺度/趋势与当前归一化/模型不匹配**。改动后你应该看到 IMF1 的 R² 和误差显著改善、日志中的警告减少，预测稳定性提高。

------

# 一、日志逐行/按段解释（重要信息）

1. `2025-10-14 ... oneDNN custom operations are on...`

   - 这是 TensorFlow 在启动时的**信息性提示**，表示启用了 oneDNN 优化（CPU 指令层面的加速）。不是错误，只是说明数值可能因不同实现有轻微差异。如果你想关掉可设置环境变量 `TF_ENABLE_ONEDNN_OPTS=0`。

2. `加载数据...` / `进行 VMD 分解...`

   - 程序读取数据并调用 VMD，正常。

3. `UserWarning: Do not pass an input_shape/input_dim argument to a layer. When using Sequential models, prefer using an Input(shape) object as the first layer in the model instead.`

   - Keras 给的警告：不要在 `Conv1D(..., input_shape=...)` 里直接传 `input_shape`。建议使用 `tf.keras.Input(shape=...)` 或把 `InputLayer` 作为第一层。当前只是警告，不会阻止训练，但最好修复以符合推荐写法。

4. `This TensorFlow binary is optimized to use available CPU instructions...`

   - 说明 TensorFlow 已编译以利用 CPU 指令集（SSE/AVX 等）。信息性提示。

5. `4/4 ━... 0s 65ms/step`

   - 这是 Keras 在执行 `predict()` 时显示的进度（4 个批次）。正常。

6. `imf_1 MSE:36.736842 RMSE:6.061092 MAE:5.982460 R2:-46.261894`

   - **关键问题**：IMF1 的 R² = -46（极端负值），说明模型对测试集的预测非常糟糕，预测误差远超基准（直接用均值预测还好过模型）。需重点排查 IMF1 的数据分布和模型契合度。

7. 再出现 `UserWarning`（重复），以及：

   ```
   WARNING:tensorflow:5 out of the last 9 calls ... triggered tf.function retracing...
   ```

   - 这是较重要的性能/正确性提示。含义：
     - 在短时间内多次对 `tf.function` 包装的函数进行“追踪”（tracing），较昂贵。
     - 常见原因：在循环中反复创建模型（或反复创建 `@tf.function`），或对 `predict()` 传入不同 shape 的输入使得 TF 每次都重建计算图。
   - 结果是性能下降，而且频繁 retrace 也可能带来隐式错误或不稳定的行为。

8. 其余 IMF 的指标：

   - imf_2 / imf_3 / imf_4 / imf_5 的 MSE、RMSE、MAE、R² 大多数是合理的（imf_2 R² ~0.963 等），说明框架的思路总体可用，但 IMFs 性能差异很大，需要针对性调整。

------

# 二、应当调整/优化的代码点（按优先级）

## 1) 输出层 activation 不应为 `sigmoid`

当前模型定义：

```py
model.add(Dense(1, activation='sigmoid'))
```

问题：

- `sigmoid` 将输出压缩到 (0,1)。虽然你对每个 IMF 做了 Min-Max 归一化到 [0,1]，但 `sigmoid` 容易饱和，且不利于回归精度（尤其当目标接近 0 或 1 边界时会受限）。
- 更常用做法：回归问题最后一层使用线性激活（默认 `activation=None` 或 `activation='linear'`）。

修复建议：

```py
model.add(Dense(1))  # 默认线性激活
```

## 2) 按 Keras 推荐使用 `Input`（消除 warning）

把模型构建改为：

```py
from tensorflow.keras import Input, Model
from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense

def build_cnn_lstm(input_shape, lstm_units=64):
    inp = Input(shape=input_shape)
    x = Conv1D(filters=32, kernel_size=3, activation='relu')(inp)
    x = MaxPooling1D(pool_size=2)(x)
    x = LSTM(lstm_units)(x)
    out = Dense(1)(x)
    model = Model(inputs=inp, outputs=out)
    model.compile(loss='mse', optimizer='adam')
    return model
```

或者在 `Sequential` 里使用 `model.add(Input(shape=input_shape))`。

## 3) 避免 `tf.function` retracing（减少警告、提升速度）

原因在你的代码中可能有两点：

- 在 `for idx in range(u.shape[0])` 循环里每次都 `model = build_cnn_lstm(...)`，这本身是合理（每个 IMF 一个模型），但 retracing 警告通常来自在同一个模型上多次调用 `predict()` 的时候传入不同形状的数组（例如 `predict_future_values` 中 `seq` 形状变化），或在短时间内重复创建 `@tf.function`（由训练/预测内部自动创建）。
   修复建议：
- 保持 predict 时输入 shape 一致。比如在 `predict_future_values` 中始终把序列 reshape 成 `(1, win, 1)`。
- 将 `predict_future_values` 里对 `seq` 的变形保证固定 shape（见下面提供的改进代码）。
- 如果 retracing 警告仍然很多，可以在 model.compile(...) 时设置 `run_eagerly=False`（默认）并确保输入稳定，或者将 `tf.function` 装饰的用户函数移出循环（但常见情况上前两点足够）。

改进 predict_future_values（示例）：

```py
def predict_future_values(model, last_sequence, future_steps):
    # last_sequence: shape (win,) or (win,1)
    seq = last_sequence.copy().reshape(1, last_sequence.shape[0], 1).astype(np.float32)
    preds = []
    for _ in range(future_steps):
        next_val = model.predict(seq, verbose=0)  # returns shape (1,1)
        preds.append(next_val[0,0])
        # append and keep sliding window
        seq = np.concatenate([seq[:,1:,:], next_val.reshape(1,1,1)], axis=1)
    return np.array(preds)
```

确保每次传入 `predict()` 的 `seq` 形状一致 `(1, win, 1)`。

## 4) 检查并修正归一化逻辑、重构预测合并部分

问题点：

- 你对每个 IMF 单独做了 min-max 归一化，这本身可以。但**合并预测时必须用每个 IMF 的反归一化后再求和**，且预测点要对齐（同一时间戳对应的 IMF 预测值相加还原最终序列）。
- 你的合并逻辑似乎用 `pred_sum` 直接相加预测数组，但长度/对齐问题会导致误差。并且你还用原始 series 重新做了一次归一化并取 test_y，这里可能导致 test 对齐偏差。

建议：

- 保存每个 IMF 的 `arr_min`, `arr_max`，对 `test_y` 和 `y_pred` 做反归一化后，基于相同的时间索引相加。
- 更稳妥：对原始 series 做一次 VMD，保存分解后每个 IMF 的训练测试索引（`create_dataset` 生成 X,Y 时的 index mapping），保证合并时每个 IMF 的 test_y 对应的时间步完全一致。

## 5) 检查 IMF1 的数据与模型适配性

IMF1 的巨大错误可能由以下原因造成（并非互斥）：

- IMF1 可能包含 **强趋势/低频分量**（幅度大、非平稳），而你的模型（短窗 CNN-LSTM + sigmoid）不适合捕捉长期趋势。
- IMF1 的值范围可能远大于其他 IMF（即便做了归一化，不当激活/损失配置仍可能导致拟合失败）。
- 训练数据量、窗口大小、模型复杂度对该 IMF 不合适（例如趋势分量需要更大的窗口或更深的 LSTM）。
- 可能出现数据异常值或 NaN。

处理建议（针对 IMF1）：

- 先画出 IMF1 的时序曲线（原始未归一化）看下其趋势/幅度/是否有异常点。
- 如果是趋势分量，考虑用更简洁的模型（例如线性/多项式回归或使用更长的 `win`），或用 `Dense` 直接拟合，或先对趋势做差分（差分后模型拟合差分，再累加恢复）。
- 尝试不对 IMF1 做 MinMax（或用 StandardScaler），并将模型输出设为线性，训练更久（更多 epoch）看效果。
- 增加 `epochs`（例如 50~100）并加 EarlyStopping 监控 val_loss。

## 6) 模型训练参数和验证

- 当前示例里 `epochs` 很小（你示例中用 5 或 10），只是演示用。实际要评估质量需要更多 epoch 和合适的 `validation_split`，以及 EarlyStopping、ModelCheckpoint。
- 增加 `batch_size` 或调整学习率也能帮助。

------

# 三、建议的具体代码改动（可直接粘贴替换）

### 改进后的 `build_cnn_lstm`（推荐）

```py
from tensorflow.keras import Input, Model
from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense

def build_cnn_lstm(input_shape, lstm_units=64):
    inp = Input(shape=input_shape)
    x = Conv1D(filters=32, kernel_size=3, activation='relu')(inp)
    x = MaxPooling1D(pool_size=2)(x)
    x = LSTM(lstm_units)(x)
    out = Dense(1)(x)  # 线性输出
    model = Model(inputs=inp, outputs=out)
    model.compile(optimizer='adam', loss='mse')
    return model
```

### 改进后的 `predict_future_values`

```py
def predict_future_values(model, sequence, future_steps):
    # sequence: shape (win,) or (win,1)
    seq = np.array(sequence).reshape(1, -1, 1).astype(np.float32)
    preds = []
    for _ in range(future_steps):
        next_val = model.predict(seq, verbose=0)  # shape (1,1)
        preds.append(next_val[0,0])
        seq = np.concatenate([seq[:, 1:, :], next_val.reshape(1,1,1)], axis=1)
    return np.array(preds)
```

### 在训练时加入 EarlyStopping（示例）

```py
from tensorflow.keras.callbacks import EarlyStopping

es = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)
model.fit(train_x, train_y, epochs=100, batch_size=32, validation_split=0.2, shuffle=False, callbacks=[es], verbose=1)
```

### 检查 IMF1 分布（建议在 main 中添加）

```py
import matplotlib.pyplot as plt

plt.figure()
plt.plot(arr)  # IMF1 原始值
plt.title('IMF1 原始序列预览')
plt.show()
print('IMF1 min/max/mean/std:', arr.min(), arr.max(), arr.mean(), arr.std())
```

------

# 四、如何验证改动生效（验证步骤）

1. 修改以上代码后，把 `Dense(1)` 替代掉 `sigmoid`，并用 `Input`/`Model` 构造网络，确保 `predict_future_values` 每次传入的 shape 恒定。
2. 增加 `epochs`（例如到 50）并使用 `EarlyStopping`（patience=8）。观察训练日志中 val_loss 是否下降稳定。
3. 运行完整流程，看以下几点：
   - `imf_1` 的 MSE/RMSE/MAE 降低，R² 从极低负值变为接近 0 或正值（越高越好）。
   - `WARNING: retracing ...` 警告显著减少或消失（主要因为 predict 调用的 shape 一致）。
   - Keras 的 UserWarning 关于 `input_shape` 消失（因为改为 Input）。

------

# 五、关于合并 IMF 与最终输出的校验

- 确保对每个 IMF 的 `test_y` 与 `y_pred` 做 **独立反归一化**：

  ```py
  test_y_orig = test_y * (arr_max - arr_min) + arr_min
  pred_orig = y_pred_flat * (arr_max - arr_min) + arr_min
  ```

- 合并时，按时间对齐将各个 IMF 的 `pred_orig` 相加（相同时间步）。不要随意截断或使用不同的索引，否则最终合成误差会很大。

- 最好把所有 IMF 的测试集长度对齐为同一个 `L`（例如取最小测试长度），再相加比较。

------

# 六、补充：如果你希望我直接替你改好并生成新 ZIP

我可以在本地（sandbox）把上述改进代码应用到你工程并打包成一个新 ZIP 给你下载（包含早停、修复 predict、线性输出等）。如果你愿意让我这样操作，请回复 “请帮我改并打包”，我会直接替你生成并提供下载链接（我会把训练 epoch 设为 10~20 以便快速测试，且保留可修改的超参）。

------

# 七、总结（要点回顾）

- 必做修改（优先级高）
  1. 把 `Dense(1, activation='sigmoid')` 改为线性输出 `Dense(1)`。
  2. 使用 `Input(shape=...)` 或 `Model` 替代直接在 Conv1D 中传 `input_shape`。
  3. 修正 `predict_future_values`，确保每次 `predict()` 输入 shape 固定 `(1, win, 1)`，以避免 retracing。
  4. 针对 IMF1：绘图并查看其分布，考虑使用更长窗口、不同预处理或不同模型（线性/差分）来拟合趋势成分。
- 可选改进
  - 增加 epochs + EarlyStopping。
  - 更严格的合并对齐逻辑（保证同一时间步相加）。
  - 考虑用标准化（StandardScaler）或按 IMF 的性质选择不同缩放策略。

------

如果你愿意，我现在可以：

- （A）把改好的代码直接打包并发给你测试（会把训练 epoch 设小以便快速试验），或者
- （B）把需要你本地替换的最小 diff（几行代码片段）贴给你，便于你手工修改并重新运行。

你选哪一个？